{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook file, I will use my own model to finish two part of work:\n",
    "First Part - Layout Recognition: Use ViT to extract main text\n",
    "Second Part - OCR: Recognize the text correctly\n",
    "\n",
    "In the notebook, the content of whole training and validation process is shown below:\n",
    "0. The environment setup\n",
    "1. Convert the pdf raw files into images and make annotations using LabelMe\n",
    "2. Use ViT to detect main text\n",
    "3. Crop the main text\n",
    "4. OCR Recognition\n",
    "5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in /home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages (1.17.0)\n",
      "Requirement already satisfied: pillow in /home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages (from pdf2image) (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "# 0. The environment prerequisites:\n",
    "!pip install pdf2image\n",
    "# install poppler\n",
    "\n",
    "# !pip install labelme # use labelme to make annotations, I use Windows to finish this step\n",
    "# But other steps are finished in Ubuntu22 (WSL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert the pdf raw files into images and make annotations using LabelMe\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "pdf_folder = \"data/raw\"\n",
    "output_root = \"data/convert_image\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "print(\"Found PDF files:\", pdf_files)\n",
    "\n",
    "dpi_value = 300\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"\\nConverting: {pdf_file}\")\n",
    "    base_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "\n",
    "    pdf_output_dir = os.path.join(output_root, base_name)\n",
    "    os.makedirs(pdf_output_dir, exist_ok=True)\n",
    "    \n",
    "    # LOAD PAGE ONE BY ONE, LOWER CONSUMPTION OF PERFORMANCE\n",
    "    chunk_size = 10\n",
    "    start_page = 1\n",
    "    end_page = chunk_size\n",
    "\n",
    "    # 6.pdf is too large for 300 dpi\n",
    "    if base_name == \"6\":  # Use 150 DPI for 6.pdf\n",
    "        dpi_value = 150\n",
    "    else:  # Keep 300 DPI for 1.pdf-5.pdf\n",
    "        dpi_value = 300\n",
    "\n",
    "    while True:\n",
    "        pages = convert_from_path(pdf_file, dpi=dpi_value,\n",
    "                                  first_page=start_page,\n",
    "                                  last_page=end_page)\n",
    "\n",
    "        if not pages:\n",
    "            break\n",
    "\n",
    "        # SAVE PAGE ONE BY ONE\n",
    "        for i, page in enumerate(pages, start=start_page):\n",
    "            out_path = os.path.join(pdf_output_dir, f\"{base_name}_page_{i:03d}.png\")\n",
    "            page.save(out_path, \"PNG\")\n",
    "            print(f\"  Saved: {out_path}\")\n",
    "\n",
    "        # GOING TO NEXT PAGE\n",
    "        start_page = end_page + 1\n",
    "        end_page += chunk_size\n",
    "\n",
    "print(\"\\nCONVERTING SUCCESSFULLY.\")\n",
    "\n",
    "# The annotations are made by LabelMe\n",
    "# Annotation files are stored in src/data/annotation\n",
    "# Annotation way screenshots are stored in docs/annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 16 samples\n",
      "Val set: 1 samples\n",
      "Test set: 1 samples\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 271, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 170\u001b[0m\n\u001b[1;32m    167\u001b[0m test_loader  \u001b[38;5;241m=\u001b[39m DataLoader(test_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# 测试一下能否正常获取一个 batch\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m sample_imgs, sample_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample images shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_imgs\u001b[38;5;241m.\u001b[39mshape)   \u001b[38;5;66;03m# [B, C, H, W]\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample masks shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_masks\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/jeffliu/miniconda3/envs/ml/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 271, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "# 2. Use ViT to detect main text\n",
    "\n",
    "# 1.1 从 JSON + 原图生成分割掩码（mask）\n",
    "# 1.2 编写一个 PyTorch Dataset，让它一次性返回 (image, mask)\n",
    "# 1.3 切分成训练集、验证集、测试集（如果数据足够多的话）\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# 1.1 从 JSON + 原图生成分割掩码\n",
    "def load_mask_from_json(json_path, image_path):\n",
    "    \"\"\"\n",
    "    读取 LabelMe 标注的 JSON 文件，生成二值分割掩码。\n",
    "    1 表示文本区域, 0 表示背景。\n",
    "    \"\"\"\n",
    "    # 读取原图尺寸\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "    height, width = img.shape[:2]\n",
    "    \n",
    "    # 创建空白 mask\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # 读取 JSON\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 遍历 shapes\n",
    "    for shape in data.get('shapes', []):\n",
    "        if shape['label'] == 'text':\n",
    "            # 提取多边形顶点\n",
    "            points = shape['points']\n",
    "            pts = np.array(points, dtype=np.int32)\n",
    "            # 在 mask 上填充多边形\n",
    "            cv2.fillPoly(mask, [pts], color=1)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "# 1.2 自定义一个 PyTorch Dataset，用于返回 (image, mask)\n",
    "class TextSegDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 收集所有图片文件名\n",
    "        self.img_files = []\n",
    "        for f in os.listdir(img_dir):\n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                self.img_files.append(f)\n",
    "        self.img_files.sort()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 根据索引拿到图片名\n",
    "        img_name = self.img_files[idx]\n",
    "        # 假设 json 与图片同名（扩展名不同）\n",
    "        json_name = img_name.rsplit('.', 1)[0] + '.json'\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        json_path = os.path.join(self.label_dir, json_name)\n",
    "        \n",
    "        # 生成 mask\n",
    "        mask = load_mask_from_json(json_path, img_path)\n",
    "        \n",
    "        # 读取图像 (BGR -> RGB)\n",
    "        img_bgr = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 转为 PIL 形式，方便用 torchvision.transforms\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "        \n",
    "        # 如果有 transform，则对图像进行 transform\n",
    "        # 同时也要对 mask 做相应的 resize 等操作\n",
    "        if self.transform is not None:\n",
    "            # 常见做法：对图像用 self.transform\n",
    "            img_t = self.transform(img_pil)\n",
    "            \n",
    "            # 如果 transform 中包含了 resize，需要同步对 mask 做同样的 resize\n",
    "            # 若 transform 仅仅是 ToTensor()，那就简单处理即可：\n",
    "            mask_t = T.ToTensor()(mask_pil)\n",
    "            \n",
    "            # 但如果 transform 有 Resize((224,224))，则 mask 也要 Resize((224,224))：\n",
    "            # 可以自定义一个 transform 或单独处理：\n",
    "            # 例如:\n",
    "            # mask_t = T.Resize((224,224))(mask_pil)\n",
    "            # mask_t = T.ToTensor()(mask_t)\n",
    "            \n",
    "        else:\n",
    "            # 否则就直接转 tensor\n",
    "            img_t = T.ToTensor()(img_pil)\n",
    "            mask_t = T.ToTensor()(mask_pil)\n",
    "        \n",
    "        # mask_t 的 shape 通常是 [1, H, W]，像素值在 [0,1]\n",
    "        # 你可以在这里做一些类型转换，比如 mask_t.float()，方便后续计算损失\n",
    "        return img_t, mask_t\n",
    "\n",
    "\n",
    "# 1.3 切分成训练集、验证集、测试集（如果数据足够多）\n",
    "def create_datasets(img_dir, label_dir, transform=None, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    根据给定目录创建数据集，并按照比例划分训练 / 验证 / 测试集。\n",
    "    val_ratio: 验证集比例\n",
    "    test_ratio: 测试集比例\n",
    "    \"\"\"\n",
    "    full_dataset = TextSegDataset(img_dir, label_dir, transform=transform)\n",
    "    total_len = len(full_dataset)\n",
    "    \n",
    "    # 计算划分大小\n",
    "    val_len = int(total_len * val_ratio)\n",
    "    test_len = int(total_len * test_ratio)\n",
    "    train_len = total_len - val_len - test_len\n",
    "    \n",
    "    # 随机划分\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, [train_len, val_len, test_len],\n",
    "        generator=torch.Generator().manual_seed(42)  # 固定随机种子，方便复现\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# ============ 以下是一个简单示例，展示如何使用 ============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设你的数据在 data/annotation_images 和 data/annotation_labels\n",
    "    img_dir = \"data/annotation_images\"\n",
    "    label_dir = \"data/annotation_labels\"\n",
    "    \n",
    "    # 例如：我们要把图像和 mask 都 resize 到 224x224，并转为 Tensor\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # 创建数据集并划分\n",
    "    train_ds, val_ds, test_ds = create_datasets(\n",
    "        img_dir=img_dir,\n",
    "        label_dir=label_dir,\n",
    "        transform=transform,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {len(train_ds)} samples\")\n",
    "    print(f\"Val set: {len(val_ds)} samples\")\n",
    "    print(f\"Test set: {len(test_ds)} samples\")\n",
    "    \n",
    "    # 你也可以直接使用全量数据，不做划分：\n",
    "    # full_dataset = TextSegDataset(img_dir, label_dir, transform=transform)\n",
    "    \n",
    "    # 用 DataLoader 来加载数据\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=2)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=2, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # 测试一下能否正常获取一个 batch\n",
    "    sample_imgs, sample_masks = next(iter(train_loader))\n",
    "    print(\"Sample images shape:\", sample_imgs.shape)   # [B, C, H, W]\n",
    "    print(\"Sample masks shape:\", sample_masks.shape)   # [B, 1, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型（ViT + Head）\n",
    "\n",
    "# Backbone（主干）：ViT 主要负责把图像“看懂”——提取特征。\n",
    "# Head（头）：指模型最后几层，用来输出你想要的结果。\n",
    "# 在分割任务里，“头”通常是一些卷积或上采样层，把特征图转换成与原图同大小的分割结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练与验证\n",
    "\n",
    "# 用你自定义的 DataLoader（基于上面的 Dataset）迭代数据。\n",
    "# 送入模型，计算损失函数（比如交叉熵、BCE 等）。\n",
    "# 反向传播、更新模型参数。\n",
    "# 观察损失是否下降，用验证集做指标（如 Dice、IoU），看效果是否提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估 & 可视化\n",
    "\n",
    "# 在测试集上推理，计算指标。\n",
    "# 可视化看看预测的分割图是否跟真实标注接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Crop the main text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. OCR Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
